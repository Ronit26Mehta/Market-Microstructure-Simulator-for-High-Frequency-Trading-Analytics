
# Market Microstructure Simulator for High-Frequency Trading Analytics

## Overview
The **Market Microstructure Simulator** is a comprehensive project designed to study and analyze high-frequency trading (HFT) systems. It aims to simulate real-world trading environments, allowing users to gain insights into the intricate mechanisms of financial markets. The project provides tools for creating synthetic market data, analyzing trading patterns, detecting anomalies, and visualizing market metrics. 

It combines advanced technologies such as:
- **Flask**: To manage dynamic APIs for data generation.
- **Dash**: For interactive visualizations and data presentation.
- **PySpark**: To handle large-scale data transformations and computations.
- **Scikit-learn**: For machine learning models applied to market analysis.
- **Dask**: For scalable data preprocessing.

This project is ideal for researchers, students, and practitioners seeking to study HFT systems and market microstructure.

---

## Features
1. **Synthetic Data Generation**:
   - A Flask API dynamically generates synthetic market data simulating real-world scenarios.

2. **Interactive Visualization**:
   - Dash-powered dashboards visualize metrics like bid-ask spreads, rolling averages, and event-driven metrics.

3. **Machine Learning Models**:
   - Algorithms for clustering (KMeans), classification (Random Forest), and anomaly detection, implemented with PySpark and Scikit-learn.

4. **Advanced Data Transformation**:
   - Leverages PySpark and Dask to handle complex data operations, such as aggregations, rolling averages, and event-based computations.

5. **Serialized Analysis Outputs**:
   - Saves processed data and intermediate results in `.pkl` format for reuse and efficient experimentation.

---

## Installation

### Prerequisites
- Python 3.8+
- pip
- Java 8 or higher (required for PySpark)

### Steps
1. Clone this repository:
   ```bash
   git clone <repository-url>
   cd Market-Microstructure-Simulator-for-High-Frequency-Trading-Analytics
   ```

2. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Start the Flask API:
   ```bash
   python API/api.py
   ```

4. Launch the dashboard:
   ```bash
   python Dashboard/dashboard.py
   ```

5. Run transformations or models using PySpark:
   ```bash
   python Transformer/sparks.py
   python Model/ml.py
   ```

---
### File Structure

The project follows a well-organized structure to streamline development and execution. Below is a detailed breakdown of the directories and files:

```
Market-Microstructure-Simulator/
├── API/
│   └── api.py                  # Flask API to handle synthetic data generation and management
├── Dashboard/
│   └── dashboard.py            # Dash-powered dashboard for data visualization
├── Data/
│   └── synthetic_market_data.csv  # Sample synthetic market data
├── Model/
│   └── ml.py                   # Machine learning models for analysis
├── Output/                     # Contains serialized outputs of analysis
│   ├── anomalies_df.pkl
│   ├── correlation_result.pkl
│   ├── event_metrics_df.pkl
│   ├── order_flow_distribution.pkl
│   ├── rolling_avg_df.pkl
│   └── weighted_avg_df.pkl
├── Transformer/
│   └── sparks.py               # PySpark-based data transformation and aggregation
├── .gitignore                  # Specifies files/directories to ignore in version control
├── README.md                   # Documentation file (this one)
├── requirements.txt            # List of dependencies for the project
```

---

### Explanation of Directories and Files
1. **`API/`**:
   - **`api.py`**:
     - Flask API to initialize, update, and serve synthetic market data.
     - Provides endpoints for data retrieval and integration with downstream components.

2. **`Dashboard/`**:
   - **`dashboard.py`**:
     - An interactive dashboard built using Dash.
     - Visualizes key metrics like anomalies, rolling averages, and event-driven patterns.

3. **`Data/`**:
   - **`synthetic_market_data.csv`**:
     - A sample CSV file generated by the API.
     - Includes columns such as timestamps, bid/ask prices, trade volume, etc.

4. **`Model/`**:
   - **`ml.py`**:
     - Houses machine learning models to analyze market data.
     - Features clustering, classification, and evaluation metrics.

5. **`Output/`**:
   - Contains serialized `.pkl` files that store intermediate and final processed data for reuse.
     - **`anomalies_df.pkl`**: Contains detected anomalies.
     - **`correlation_result.pkl`**: Stores correlation analysis results.
     - **`event_metrics_df.pkl`**: Event-based computed metrics.
     - **`order_flow_distribution.pkl`**: Distribution patterns of order flows.
     - **`rolling_avg_df.pkl`**: Rolling averages of market metrics.
     - **`weighted_avg_df.pkl`**: Weighted averages for analysis.

6. **`Transformer/`**:
   - **`sparks.py`**:
     - PySpark-based script for advanced data transformations.
     - Handles data fetching, rolling averages, and large-scale aggregations.

7. **Root Files**:
   - **`.gitignore`**: Defines files/directories to be ignored by Git.
   - **`README.md`**: Project documentation.
   - **`requirements.txt`**: Lists Python dependencies needed for the project.



## File Structure and Explanation

### **Root Directory**
- **`.gitignore`**: Lists files and directories ignored by Git.
- **`README.md`**: The documentation file (this one).
- **`requirements.txt`**: Contains a list of all required Python dependencies.

### **API**
- **`api.py`**:
  - Implements a Flask-based API to handle synthetic market data.
  - Features:
    - **Data Initialization**: Creates a CSV file to store market data if it doesn’t exist.
    - **Threaded Updates**: Dynamically updates data using threading.
    - **Endpoints**: Provides routes for fetching and managing data.

### **Dashboard**
- **`dashboard.py`**:
  - Creates an interactive dashboard using Dash.
  - Loads pickled datasets (e.g., rolling averages, anomalies).
  - Displays visualizations such as:
    - Weighted averages
    - Anomalies over time
    - Event-based metrics

### **Data**
- **`synthetic_market_data.csv`**:
  - A sample dataset generated by the API simulating market events.
  - Contains columns such as timestamp, bid/ask prices, and trade volume.

### **Model**
- **`ml.py`**:
  - Implements machine learning models for market data analysis.
  - Key functionalities:
    - **Classification**: Uses Random Forest and Gradient Boosted Trees.
    - **Clustering**: Employs KMeans for market segmentation.
    - **Evaluation**: Includes metrics like confusion matrices and classification reports.

### **Transformer**
- **`sparks.py`**:
  - Handles data transformation using PySpark.
  - Key functionalities:
    - **Data Fetching**: Fetches data from the Flask API.
    - **Data Aggregation**: Computes rolling averages, weighted metrics, and other aggregations.
    - **Data Integration**: Combines results for machine learning input.

### **Output**
- Stores serialized data in `.pkl` format for reuse. Files include:
  - **`anomalies_df.pkl`**: Detected anomalies.
  - **`correlation_result.pkl`**: Results of correlation analysis.
  - **`event_metrics_df.pkl`**: Event-driven metrics.
  - **`order_flow_distribution.pkl`**: Distribution of order flows.
  - **`rolling_avg_df.pkl`**: Computed rolling averages.
  - **`weighted_avg_df.pkl`**: Weighted averages of market metrics.

---

## Usage

### 1. Start the Flask API
Run the API to generate and manage synthetic market data:
```bash
python API/api.py
```

### 2. Visualize Data
Launch the dashboard to view metrics and patterns:
```bash
python Dashboard/dashboard.py
```
Open the dashboard in your browser at `http://127.0.0.1:8050`.

### 3. Data Transformation
Use PySpark to transform and preprocess data:
```bash
python Transformer/sparks.py
```

### 4. Run Machine Learning Models
Analyze market patterns using the `ml.py` script:
```bash
python Model/ml.py
```

---

## Dependencies
Key libraries and tools:
- **Flask**: Backend for API.
- **Dash**: Interactive visualization.
- **PySpark**: Scalable data processing and machine learning.
- **Dask**: Parallel processing for large datasets.
- **Plotly**: Data visualization.
- **Scikit-learn**: Machine learning.
- **Pandas**: Data manipulation.
- **Pickle**: For serialization of outputs.

Refer to `requirements.txt` for the complete list.

---

## Future Enhancements
- Integration with live market data sources.
- Real-time trading simulation.
- Deployment-ready dashboards and APIs.
- Exploration of deep learning models for predictive analytics.
- Multi-threaded data ingestion pipelines.

---
## Outputs:-
Transformed Data Obtained from the pyspark:
1. Correlation and Distribution of order flow:
   ![Screenshot 2024-11-19 144051](https://github.com/user-attachments/assets/c02a3a65-d313-4090-b86e-c0a47ed34113)
2.Rolling Average:
  ![Screenshot 2024-11-19 144004](https://github.com/user-attachments/assets/437f31ce-4c01-45ee-b78a-f20c65a50169)
3.Event Based Metrics:
  ![Screenshot 2024-11-19 143914](https://github.com/user-attachments/assets/81c2a8dd-331e-4468-b1d8-5cf7688990c5)
4.Anomalies:
  ![Screenshot 2024-11-19 143830](https://github.com/user-attachments/assets/66689747-c94e-4c1c-bad1-518cde84c8da)
5.Weighted Average Prices:
  ![Screenshot 2024-11-19 143732](https://github.com/user-attachments/assets/1f8cde17-1be9-4ff6-9fe4-0ab9b1385b62)










